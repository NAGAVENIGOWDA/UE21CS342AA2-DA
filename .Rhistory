data$Hail_Accident_Occur <- ifelse(data$`Hail/Sleet - Total Accidents` > 0, 1, 0)
# Calculate biserial correlation
correlation <- biserial.cor(data$`Rainy - Total Accidents`, data$Hail_Accident_Occur)
# Print the correlation coefficient
print(correlation)
library(psych)
data$DustStormOccurred <- ifelse(df$Dust.Storm...Total.Accidents > 0, 1, 0)
library(psych)
data$DustStormOccurred <- ifelse(data$`Dust Storm - Total Accidents` > 0, 1, 0)
data$HailSleetOccurred <- ifelse(data$`Hail/Sleet - Total Accidents` > 0, 1, 0)
contingency_table <- table(data$DustStormOccurred, data$HailSleetOccurred)
print(contingency_table)
phi_coefficient <- phi(contingency_table)
cat("Phi Coefficient:", phi_coefficient, "\n")
install.packages('tinytex')
tinytex::install_tinytex()
install.packages("tinytex")
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
data <- read_excel("C:/Users/Praka/OneDrive/Documents/5thSem/DA/students.xlsx")
str(data)
# Install and load necessary packages
if (!requireNamespace("tidyverse", quietly = TRUE)) {
install.packages("tidyverse")
}
if (!requireNamespace("moments", quietly = TRUE)) {
install.packages("moments")
}
library(tidyverse)
library(moments)
library(ggplot2)
data <- read.csv("students.csv")
library(ggplot2)
boxplot(data$A, data$B, data$C, data$D,
names = c("A", "B", "C", "D"),
main = "Box Plots for Fitness Plans A, B, C, D",
xlab = "Fitness Plans",
ylab = "Data Value")
library(e1071)
install.packages("e1071")
library(e1071)
skewness_A <- skewness(data$A)
skewness_B <- skewness(data$B)
skewness_C <- skewness(data$C)
skewness_D <- skewness(data$D)
cat("Skewness for Group A:", skewness_A, "\n")
cat("Skewness for Group B:", skewness_B, "\n")
cat("Skewness for Group C:", skewness_C, "\n")
cat("Skewness for Group D:", skewness_D, "\n")
#Q-Q plot for Group A
qqnorm(data$A)
qqline(data$A, col = 2)
#Q-Q plot for Group B
qqnorm(data$B)
qqline(data$B, col = 4)
#Q-Q plot for Group C
qqnorm(data$C)
qqline(data$C, col = 7)
#Q-Q plot for Group D
qqnorm(data$D)
qqline(data$D, col = 5)
library(reshape2)
install.packages("reshape2")
library(reshape2)
data_long <- melt(data)
colnames(data_long) <- c("FitnessPlan", "Marks")
model <- aov(Marks ~ FitnessPlan, data = data_long)
anova_result <- summary(model)
print(anova_result)
pet_data <- read_excel("C:/Users/Praka/OneDrive/Documents/5thSem/DA/pet_training.xlsx")
summary(pet_data)
boxplot(ResponseTime ~ Task, data = pet_data)
boxplot(ResponseTime ~ Treat, data = pet_data)
# ANOVA for Task
task_anova <- aov(ResponseTime ~ Task, data = pet_data)
summary(task_anova)
# ANOVA for Treat
treat_anova <- aov(ResponseTime ~ Treat, data = pet_data)
summary(treat_anova)
task_posthoc <- TukeyHSD(task_anova)
print(task_posthoc)
treat_posthoc <- TukeyHSD(treat_anova)
print(treat_posthoc)
install.packages("rmarkdown")
install.packages("rmarkdown")
rmarkdown::render("NAGAVENI_LG_PES2UG21CS315_Worksheet_1_part1.Rmd", output_format = "pdf_document")
install.packages("tinytex")
data$release_date <- as.Date(data$release_date, format = "%Y-%m-%d")
knitr::opts_chunk$set(echo = TRUE)
data$release_date <- as.Date(data$release_date, format = "%Y-%m-%d")
data$release_date <- as.Date(data$release_date, format = "%Y-%m-%d")
# Convert the release_date column to Date format
data$release_date <- as.Date(data$release_date, format = "%Y-%m-%d")
# Convert the release_date column to Date format
data$release_date <- as.Date(data$release_date, format = "%d-%m-%Y")
# Example: Convert "MM/DD/YYYY" to "YYYY-MM-DD"
data$release_date <- gsub("^(\\d{2})/(\\d{2})/(\\d{4})$", "\\3-\\1-\\2", data$release_date)
data$release_date <- as.Date(data$release_date)
data$release_date <- as.Date(data$release_date)
library(lubridate)
data$release_date <- as.Date(data$release_date)
library(lubridate)
data$release_date <- as.Date(data$release_date)
library(lubridate)
data$release_date <- as.Date(data$release_date, format = "%Y-%m-%d", na.rm = TRUE)
data$release_date <- as.Date(data$release_date)
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
# Read Excel file
data <- read_excel("C:/Users/Praka/OneDrive/Documents/5thSem/DA/Dataset_2a.xlsx")
library(readxl)
# Read Excel file
data <- read.csv("Dataset_2a.csv")
library(readxl)
# Read Excel file
data <- read_excel("C:/Users/Praka/OneDrive/Documents/5thSem/DA/Dataset_2a.xlsx")
library(readxl)
setwd("C:/Users/Praka/OneDrive/Documents/5thSem/DA")
# Read Excel file
data <- read_excel("C:/Users/Praka/OneDrive/Documents/5thSem/DA/Dataset_2a.xlsx")
library(readxl)
setwd("C:/Users/Praka/OneDrive/Documents/5thSem/DA")
# Read Excel file
# Using forward slashes and double-checking the path
path <- "C:/Users/Praka/OneDrive/Documents/5thSem/DA/Dataset_2a.xlsx"
# Reading the Excel file
data <- read_excel(path)
library(readxl)
setwd("C:/Users/Praka/OneDrive/Documents/5thSem/DA")
# Read Excel file
# Using forward slashes and double-checking the path
path <- "C:/Users/Praka/OneDrive/Documents/5thSem/DA/33.-34.Dataset_2a.xlsx"
# Reading the Excel file
data <- read_excel(path)
# Set the working directory
setwd("C:/Users/Praka/OneDrive/Documents/5thSem/DA")
# Specify the CSV file path
path <- "C:/Users/Praka/OneDrive/Documents/5thSem/DA/Dataset_2a.csv"
# Read the CSV file
data <- read.csv(path)
# Check the structure of the loaded data
str(data)
# Set the working directory
setwd("C:/Users/Praka/OneDrive/Documents/5thSem/DA")
# Specify the CSV file path
path <- "C:/Users/Praka/OneDrive/Documents/5thSem/DA/Dataset_2a.csv"
# Read the CSV file
data <- read.csv(path)
# Check the structure of the loaded data
head(data)
library(ggplot2)
ggplot(data, aes(x=horsepower, y=price)) +
geom_point() +
geom_smooth(method="lm")
#plotting residuals
model <- lm(price ~ horsepower, data=data)
# Create scatter plot with residuals
ggplot(data, aes(x=horsepower, y=resid(model))) +
geom_point() +
geom_hline(yintercept=0, linetype="dashed")
data
library(ggplot2)
library(glmnet)
installed.packages("glmnet")
library(ggplot2)
library(glmnet)
install.packages("glmnet")
library(ggplot2)
library(glmnet)
# Define the data frame and variables
df <- data # Replace 'data' with the actual name of your data frame
x <- df$horsepower
y <- df$price
# Define the learning rate and number of iterations
learning_rate <- 0.01
num_iterations <- 1000
# Initialize the coefficients
intercept <- 0
slope <- 0
residuals <- c()
# Perform gradient descent
n <- length(x)
for (i in 1:num_iterations) {
y_pred <- intercept + slope * x
residuals <- c(residuals, y_pred-y)
intercept <- intercept - learning_rate * (2/n) * sum(residuals)
slope <- slope - learning_rate * (2/n) * sum(residuals * x)
}
# Plot residuals using ggplot
ggplot(data = data.frame(x = x, residuals = residuals), aes(x = x, y = residuals)) +
geom_point() +
labs()
data <- read.csv("Dataset_2a.csv")
data$drivewheel_encoded <- ifelse(data$drivewheel == "fwd", 1, 0)
# Define the data frame and variables
df <- data # Replace 'data' with the actual name of your data frame
x <- df$drivewheel_encoded
y <- df$price
# Normalize the 'y' variable
y <- (y - min(y)) / (max(y) - min(y))
# Perform logistic regression
fit <- glm(y ~ x, data = df, family = binomial())
# Predict the probabilities
probabilities <- predict(fit, type = "response")
# Convert probabilities to predicted classes
predicted_classes <- ifelse(probabilities > 0.5, "fwd", "rwd")
# Create a new data frame with the predicted classes
df_predicted <- data.frame(drivewheel_encoded = x, price = y, predicted_class = predicted_classes)
# Plot the predicted classes
ggplot(data = df_predicted, aes(x = drivewheel_encoded, y = price, color = predicted_class)) +
geom_point() +
labs(x = "Drive Wheel Encoded", y = "Normalized Price")
library(pROC)
install.packages("pROC")
library(pROC)
roc_obj <- roc(y, probabilities)
# Plot the ROC curve
plot(roc_obj)
data <- read.csv("Dataset_2a.csv")
library(dplyr)
data <- data %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.numeric))
data
library(corrplot)
corrplot(cor(data), method = "circle")
library(corrplot)
library(dplyr)
# Filter columns with correlation >0.5 or <-0.5 with price
correlations <- cor(data)
columns_to_keep <- which(abs(correlations[, "price"]) > 0.5)
filtered_data <- data[, columns_to_keep]
# Create MLR model
model <- lm(price ~ ., data = filtered_data)
columns_to_keep
knitr::opts_chunk$set(echo = TRUE)
cars <- read.csv('Dataset_2a.csv')
head(cars)
library(ggplot2)
library(gridExtra)
plot1 <- ggplot(cars, aes(x = price)) +
geom_density() +
labs(title = "Car Price Distribution Plot")
# Create the second plot (Car Price Spread)
plot2 <- ggplot(cars, aes(y = price)) +
geom_boxplot() +
labs(title = "Car Price Spread")
# Combine the plots and display
grid.arrange(plot1, plot2, ncol = 2)
library(ggplot2)
subset_data <- cars[, c("horsepower", "price")]
scatter_plot <- ggplot(subset_data, aes(x = horsepower, y = price)) +
geom_point() +  # Scatter plot
geom_smooth(method = "lm", se = FALSE, color = "red") +  # Best-fit line
labs(x = "Horsepower (hp)", y = "Price (USD)") +  # Axis labels
ggtitle("Scatter Plot of Horsepower vs. Price")  # Title
print(scatter_plot)
# he scatter plot reveals a clear positive correlation between horsepower and price, implying that vehicles with higher horsepower generally come with a higher price tag.
subset_data$residuals <- resid(lm(price ~ horsepower, data = subset_data))
residuals_plot <- ggplot(subset_data, aes(x = horsepower, y = residuals)) +
geom_point() +  # Scatter plot
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +  # Add a dashed line at y=0
labs(x = "Horsepower (hp)", y = "Residuals") +  # Axis labels
ggtitle("Scatter Plot of Residuals vs. Horsepower")  # Title
# Display the residuals plot
print(residuals_plot)
#The scatter plot indeed exhibits the anticipated positive association between horsepower and price. Nonetheless, there are several data points that appear as outliers. These outliers might result from various factors, such as data inaccuracies or modified vehicles. The code will generate a scatter plot of the residuals. If these residuals are distributed randomly around the y=0 line, it suggests that the linear regression model adequately represents the data. However, the presence of any discernible pattern within the residuals would indicate that the model is not a suitable fit for the data.
#In this case, the residuals do not appear to be randomly scattered around the line y=0. There is a positive trend in the residuals, which indicates that the model is underestimating the price of cars with high horsepower.
#Addressing the challenge posed by outliers and a noticeable trend in the residuals offers several approaches. One effective approach involves data transformation. Various transformations are available, including log transformation, Box-Cox transformation, and power transformation. The most suitable transformation method depends on the specific characteristics of the data at hand.
subset_data$log_horsepower <- log(subset_data$horsepower)
subset_data$log_price <- log(subset_data$price)
scatter_plot_log <- ggplot(subset_data, aes(x = log_horsepower, y = log_price)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE, color = "red") +
labs(x = "Log(Horsepower)", y = "Log(Price)") +
ggtitle("Scatter Plot of Log(Horsepower) vs. Log(Price)")
print(scatter_plot_log)
cars <- read.csv('33.-34.Dataset_2a.csv')
cars <- read.csv('33.-34.Dataset_2a.csv')
#The outcomes derived from the Welch Two Sample t-test, illustrating a significant price difference favoring rear-wheel drive (RWD) cars over front-wheel drive (FWD) cars in the American market, bring both positive and negative implications for DATA Motors. Here's an exploration of these mixed implications:
#Advantages- Profit Potential,Revenue Generation:
#disadvantage-Market Entry Costs,Segmentation
library(ggplot2)
library(pROC)
library(dplyr)
cars <- read.csv("33.-34.Dataset_2a.csv")
#The outcomes derived from the Welch Two Sample t-test, illustrating a significant price difference favoring rear-wheel drive (RWD) cars over front-wheel drive (FWD) cars in the American market, bring both positive and negative implications for DATA Motors. Here's an exploration of these mixed implications:
#Advantages- Profit Potential,Revenue Generation:
#disadvantage-Market Entry Costs,Segmentation
library(ggplot2)
library(pROC)
library(dplyr)
cars <- read.csv("Dataset_2a.csv")
cars <- cars %>%
mutate(premium = ifelse(drivewheel == "fwd", 1, 0))
logistic_model <- glm(premium ~ price, data = cars, family = "binomial")
summary(logistic_model)
roc_obj <- roc(cars$premium, logistic_model$fitted.values)
auc_value <- auc(roc_obj)
cat("AUC (Area Under the Curve):", auc_value, "\n")
library(dplyr)
library(corrplot)
library(dplyr)
cars <- cars %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.numeric))
correlation_matrix <- cor(cars)
corrplot(correlation_matrix, method = "color", type = "upper", tl.col = "black", tl.srt = 45)
independent_variables <- cars %>%
select(-price)
correlation_matrix_independent <- cor(independent_variables)
print(correlation_matrix_independent)
library(corrplot)
corrplot(correlation_matrix_independent, method = "color", type = "upper", tl.col = "black", tl.srt = 45)
full_model <- lm(price ~ ., data = cars)
summary(full_model)
significant_vars <- c("fueltype", "aspiration", "horsepower", "curbweight", "carwidth")
best_r_squared <- 0
best_model <- NULL
best_variables <- NULL
models <- list()
for (i in 1:length(significant_vars)) {
formula <- paste("price ~", paste(significant_vars[1:i], collapse = "+"))
model <- lm(formula, data = cars)
r_squared <- summary(model)$r.squared
if (r_squared > best_r_squared) {
best_r_squared <- r_squared
best_model <- model
best_variables <- significant_vars[1:i]
}
}
cat("Best Fitting Model R-squared:", best_r_squared, "\n")
cat("Best Variables:", paste(best_variables, collapse = ", "), "\n")
summary(best_model)
#As evident from the analysis, the model incorporating all five variables exhibits the smallest residual standard error and the highest F-statistic score, indicating its superior fit for the dataset.
#Taking into account the correlogram and the F-statistic results across various models, I deduce that the multiple linear regression model encompassing all five variables (horsepower, curbweight, wheelbase, carlength, and carwidth) provides the most suitable representation for the given dataset. This model not only boasts the lowest residual standard error but also attains the highest F-statistic score.
#It's worth noting that the key predictors for car price estimation are horsepower, curbweight, and wheelbase. These variables display the strongest correlations with price and exhibit relatively low intercorrelations among themselves.
#In my most optimal model, I incorporated all five variables, which include:
#1. horsepower
#2. curbweight
#3. wheelbase
#4. carlength
#5. carwidth
data <- read.csv('waves.csv')
data <- read.csv('waves.csv')
# head(data)
summary(data)
# Split the data into training and testing sets using the indices (first 80% for training and next 20% for test)
set.seed(123)
percentage <- 0.8
rows <- nrow(data)
numeric_train <- round(percentage * rows)
num_test <- rows - numeric_train
train_index <- sample(1:rows, numeric_train)
test_index <- setdiff(1:rows, train_index)
train_data <- data[train_index, ]
test_data <- data[test_index, ]
# Load required libraries
library(corrplot)
library(ggplot2)
correlation_matrix <- cor(train_data)
corrplot(correlation_matrix, method = "color", type = "upper",
tl.col = "blue", tl.srt = 40, diag = FALSE)
WVHT_correlation <- correlation_matrix[,"WVHT"]
correlation_sort <- sort(WVHT_correlation, decreasing = TRUE)
top_6 <- names(correlation_sort[2:10])
cat(top_6)
# Create the MLR model with the most suitable parameters (There seem to be 4 right? Or is it 6?)
model <- lm(WVHT ~ APD + DPD + GST + WSPD, data = train_data)
summary(model)
# Write your code here
data_df <- data.frame(matrix(ncol = 0, nrow = 777))
predicted_values <- predict(model, newdata = test_data)
residuals <- test_data$WVHT - predicted_values
residual_sum_of_squares <- sum(residuals^2)
mean_WVHT <- mean(test_data$WVHT)
deviations_from_mean <- test_data$WVHT - mean_WVHT
total_sum_of_squares <- sum(deviations_from_mean^2)
R_squared <- 1 - (residual_sum_of_squares / total_sum_of_squares)
cat("R-squared (R2) statistic:", R_squared, "\n")
# Write your code here
library(glmnet)
X_train_data <- as.matrix(train_data[, c("APD", "DPD", "GST", "WSPD")])
y_train_data <- train_data$WVHT
ridge_regression_model <- glmnet(X_train_data, y_train_data, alpha = 0, lambda = 0.5)
coef(ridge_regression_model)
X_test_data <- as.matrix(test_data[, c("APD", "DPD", "GST", "WSPD")])
predictions <- predict(ridge_regression_model,s = 0.01, newx = X_test_data)
# Calculate R^2 here
X_test_data<- as.matrix(test_data[, c("APD", "DPD", "GST", "WSPD")])
predicted_values <- predict(ridge_regression_model,s = 0.01, newx = X_test_data)
actual_values <- test_data$WVHT
mse <- mean((actual_values - predicted_values)^2)
rmse <- sqrt(mse)
ss_total <- sum((actual_values - mean(actual_values))^2)
ss_residual <- sum((actual_values - predictions)^2)
r_squared <- 1 - (ss_residual / ss_total)
cat("R-squared (R2):", r_squared, "\n")
# Write your code here
lambda_values <- 10^seq(3, -2, length = 100)
cv_model <- cv.glmnet(X_train_data, y_train_data, alpha = 0, lambda = lambda_values)
optimal_lambda <- cv_model$lambda.min
cat("Optimal Lambda:", optimal_lambda, "\n")
# Write your code here
newridge_model <- glmnet(X_train_data, y_train_data, alpha = 0, lambda = optimal_lambda)
coef(newridge_model)
# Write your code here
library(glmnet)
X_train_data <- as.matrix(train_data[, c("APD", "DPD", "GST", "WSPD")])
y_train_data <- train_data$WVHT
ridge_regression_model <- glmnet(X_train_data, y_train_data, alpha = 0, lambda = 0.5)
coef(ridge_regression_model)
X_test_data <- as.matrix(test_data[, c("APD", "DPD", "GST", "WSPD")])
predictions <- predict(ridge_regression_model,s = 0.01, newx = X_test_data)
# Calculate R^2 here
X_test_data<- as.matrix(test_data[, c("APD", "DPD", "GST", "WSPD")])
predicted_values <- predict(ridge_regression_model,s = 0.01, newx = X_test_data)
actual_values <- test_data$WVHT
mse <- mean((actual_values - predicted_values)^2)
rmse <- sqrt(mse)
ss_total <- sum((actual_values - mean(actual_values))^2)
ss_residual <- sum((actual_values - predictions)^2)
r_squared <- 1 - (ss_residual / ss_total)
cat("R-squared (R2):", r_squared, "\n")
# Write your code here
lambda_values <- 10^seq(3, -2, length = 100)
cv_model <- cv.glmnet(X_train_data, y_train_data, alpha = 0, lambda = lambda_values)
optimal_lambda <- cv_model$lambda.min
cat("Optimal Lambda:", optimal_lambda, "\n")
# Write your code here
newridge_model <- glmnet(X_train_data, y_train_data, alpha = 0, lambda = optimal_lambda)
coef(newridge_model)
data <- read.csv('waves.csv')
# Write your code here
newridge_model <- glmnet(X_train_data, y_train_data, alpha = 0, lambda = optimal_lambda)
coef(newridge_model)
# Write your code here
library(glmnet)
X_train_data <- as.matrix(train_data[, c("APD", "DPD", "GST", "WSPD")])
y_train_data <- train_data$WVHT
ridge_regression_model <- glmnet(X_train_data, y_train_data, alpha = 0, lambda = 0.5)
coef(ridge_regression_model)
X_test_data <- as.matrix(test_data[, c("APD", "DPD", "GST", "WSPD")])
predictions <- predict(ridge_regression_model,s = 0.01, newx = X_test_data)
# Write your code here
lambda_values <- 10^seq(2, -4, length = 1000)
cv_model <- cv.glmnet(X_train_data, y_train_data, alpha = 0, lambda = lambda_values)
optimal_lambda <- cv_model$lambda.min
cat("Optimal Lambda:", optimal_lambda, "\n")
# Write your code here
lasso_regression_model <- glmnet(x = as.matrix(train_data[, -12]), y = train_data$WVHT, alpha = 1, lambda = 0.001, standardize = TRUE)
coef(lasso_regression_model, s = 0.001)
#Write your code here
model_df <- data.frame(predicted_lasso = numeric(nrow(test_data)))
model_df$predicted_lasso <- predict(lasso_regression_model, newx = as.matrix(subset(test_data, select = -c(WVHT))))
mean_observed <- mean(test_data$WVHT)
tss <- sum((test_data$WVHT - mean_observed)^2)
rss <- sum((model_df$predicted_lasso - test_data$WVHT)^2)
r2 <- 1 - (rss / tss)
cat("The predicted r2 value for lasso regression is:", r2, "\n")
# Write your code here
library(caret)
install.packages("caret")
# Write your code here
library(caret)
set.seed(12)
custom <- trainControl(method = "repeatedcv",
number = 3,
repeats = 5,
verboseIter = FALSE)
elasticnet <- train(WVHT~.,
train_data,
method='glmnet',
tuneGrid =expand.grid(alpha=seq(0,1,length=10),
lambda = seq(0.001,0.2,length=10)),
trControl=custom)
coef(elasticnet$finalModel, elasticnet$finalModel$lambdaOpt)
model_df$predicted_elasticnet <- predict(elasticnet, newdata = test_data)
mean_observed <- mean(test_data$WVHT)
tss <- sum((test_data$WVHT - mean_observed)^2)
rss <- sum((model_df$predicted_elasticnet - test_data$WVHT)^2)
r2 <- 1-(rss/tss)
cat("The predicted r2 value for ridge regression is:",r2)
alpha_value <- elasticnet$bestTune$alpha
# Access the lambda value (regularization strength)
lambda_value <- elasticnet$bestTune$lambda
# Print the values
cat("\n")
cat("Alpha Value:", alpha_value, "\n")
cat("Lambda Value:", lambda_value, "\n")
knitr::opts_chunk$set(echo = TRUE)
#Changing the dates to the correct format
df <- read.csv('water.csv')
convert_date_format <- function(date) {
date_components <- strsplit(date, "/")
new_date <- paste0(date_components[[1]][3], "-", date_components[[1]][2], "-", date_components[[1]][1])
return(new_date)
}
for(i in 1:length(df$date))
df$date[i] <- convert_date_format(df$date[i])
df$date<- as.Date(df$date)
df$date
actual_values <- ts_groundwater_level[45:66]
calculate_metrics <- function(actual_values, predicted_values) {
mae_value <- mean(abs(actual_values - predicted_values))
mape_value <- mean(abs((actual_values - predicted_values) / actual_values)) * 100
mse_value <- mean((actual_values - predicted_values)^2)
rmse_value <- sqrt(mse_value)
return(list(MAE = mae_value, MAPE = mape_value, MSE = mse_value, RMSE = rmse_value))
}
library(tseries)
adf_test <- adf.test(gwl_ts)
