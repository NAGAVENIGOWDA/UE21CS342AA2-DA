{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db054c51f95e1fe5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Worksheet 3b - ARIMAX, SARIMAX and LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d01ae52c71953",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Designed by Prateek Rao - xrprateek@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b90052cca54e2d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In the previous worksheet, we experimented with ARIMA models. However, one caveat of ARIMA (or similar models), is that it takes only the target variable into consideration, according to the timestamp. In essence, it derives the relationship between the current target variable values and the past variable values.\n",
    "\n",
    "However, what if we have some other external factors affecting the target values?\n",
    "This is where *ARIMAX* (AutoRegressive Integrated Moving Average with eXogenous variables) steps in!\n",
    "\n",
    "ARIMAX extends the capabilities of ARIMA by incorporating external factors or exogenous variables that influence the time series data. It's the bridge that connects the simplicity of ARIMA with the complexity of real-world forecasting, allowing us to tackle more intricate and realistic forecasting challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73023a00b371eae",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3180d75ea831c0c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Make sure you have the following libraries installed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f5544802e07b9e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:12:28.590782Z",
     "start_time": "2023-09-25T18:12:28.208299Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.13.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.24.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: numpy>=1.18 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from statsmodels) (1.24.3)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.4 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from statsmodels) (1.11.1)\n",
      "Requirement already satisfied: pandas>=1.0 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from statsmodels) (2.1.1)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from statsmodels) (0.5.3)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from statsmodels) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=21.3->statsmodels) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.0->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.0->statsmodels) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.0->statsmodels) (2023.3)\n",
      "Requirement already satisfied: six in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install keras\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0101a14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a05a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\praka\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb22e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5b097b7c967ff2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:12:28.599156Z",
     "start_time": "2023-09-25T18:12:28.213009Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Praka\\OneDrive\\Documents\\5thSem\\DA\\PES1UG21CS636_Worksheet3B.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Praka/OneDrive/Documents/5thSem/DA/PES1UG21CS636_Worksheet3B.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m MinMaxScaler\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Praka/OneDrive/Documents/5thSem/DA/PES1UG21CS636_Worksheet3B.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m mean_squared_error, mean_absolute_error\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Praka/OneDrive/Documents/5thSem/DA/PES1UG21CS636_Worksheet3B.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Praka/OneDrive/Documents/5thSem/DA/PES1UG21CS636_Worksheet3B.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Praka/OneDrive/Documents/5thSem/DA/PES1UG21CS636_Worksheet3B.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m LSTM\n",
      "File \u001b[1;32mc:\\Users\\Praka\\OneDrive\\Documents\\5thSem\\DA\\myenv\\lib\\site-packages\\keras\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m __internal__\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m activations\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m applications\n",
      "File \u001b[1;32mc:\\Users\\Praka\\OneDrive\\Documents\\5thSem\\DA\\myenv\\lib\\site-packages\\keras\\__internal__\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m \u001b[39mimport\u001b[39;00m layers\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m \u001b[39mimport\u001b[39;00m losses\n",
      "File \u001b[1;32mc:\\Users\\Praka\\OneDrive\\Documents\\5thSem\\DA\\myenv\\lib\\site-packages\\keras\\__internal__\\backend\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m _initialize_variables \u001b[39mas\u001b[39;00m initialize_variables\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m track_variable\n",
      "File \u001b[1;32mc:\\Users\\Praka\\OneDrive\\Documents\\5thSem\\DA\\myenv\\lib\\site-packages\\keras\\src\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Input\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n",
      "File \u001b[1;32mc:\\Users\\Praka\\OneDrive\\Documents\\5thSem\\DA\\myenv\\lib\\site-packages\\keras\\src\\models\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m Functional\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n",
      "File \u001b[1;32mc:\\Users\\Praka\\OneDrive\\Documents\\5thSem\\DA\\myenv\\lib\\site-packages\\keras\\src\\engine\\functional.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdtensor\u001b[39;00m \u001b[39mimport\u001b[39;00m layout_map \u001b[39mas\u001b[39;00m layout_map_lib\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import to_datetime\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d27f4431df5889",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f5b2793bba284d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The dataset `electricity.csv` has been given to you. Let's load it using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a99eebcb92738",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:12:28.873539Z",
     "start_time": "2023-09-25T18:12:28.225563Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\raka\\\\OneDrive\\\\Documents\\\\5thSem\\\\DA\\\\electricity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f6361d6235450",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:12:28.877487Z",
     "start_time": "2023-09-25T18:12:28.233165Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139be428db7c4dff",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Data Dictionary:\n",
    "\n",
    "* `power_consumed`: *Target variable*. Electricity consumed by a household.\n",
    "* `weather_index` : A cluster assigned to type of weather, through previous preprocessing. For instance, cloudy could be 1, sunny could be 2, etc.\n",
    "* `holiday_index` : 0 if it's a working day, 1 if it's a holiday.\n",
    "* `maximum_temperature` : Max temperature of the particular day. \n",
    "* `humidity` : Humidity measured as dew point.\n",
    "* `wind_velocity` : Average wind speed.\n",
    "* `pressure` : Atmospheric pressure in the locality.\n",
    "* `rainfall` : Average rainfall amount."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0deb34ec74c269",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We intend to perform electricity usage forecasting over every single day, used by a household. At this stage, a suggestion would be to start thinking about your knowledge with respect to electricity bills. Are there specific months when you'd expect higher power consumption over some other months? Do you think this could be effectively modeled with some time series methods that you have learnt of, in your course?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a7b384ee8df179",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Remember, we are performing time series analysis here. A general rule of thumb is to have the `time` column as the index column!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b0850e2fa8a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:12:28.885604Z",
     "start_time": "2023-09-25T18:12:28.243949Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df['day'] = pd.to_datetime(df['day'])\n",
    "df.set_index(['day'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c436eb28f5a08ad7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Before we try to forecast anything, let's go ahead and split the dataset into train-test sets, as discussed in the previous worksheet. \n",
    "Remember, since we're dealing with time series data, we will not perform a random split! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc01c96067c3f440",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:12:28.892123Z",
     "start_time": "2023-09-25T18:12:28.247238Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train = df.iloc[0:(len(df)-30)]\n",
    "test = df.iloc[len(train):(len(df)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4094f32147dc6c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Since this is a small dataset, and is being used only for demonstration purposes, we've used a split of only 30 instances for testing and the rest for training.\n",
    "\n",
    "In the real world, we'd prefer using a 70-15-15 % split for train-test-val, or 80-20 for train-test, as required by the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe80453e75e030b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:12:28.895646Z",
     "start_time": "2023-09-25T18:12:28.252295Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7aeafdbc9c039a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:12:29.058517Z",
     "start_time": "2023-09-25T18:12:28.261604Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train['power_consumed'].plot(figsize=(25,5))\n",
    "test['power_consumed'].plot(figsize=(25,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff31ade48f1c584",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Augmented Dickey-Fuller Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e797546aef39f5ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:12:29.058747Z",
     "start_time": "2023-09-25T18:12:28.441214Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "t = sm.tsa.adfuller(train.power_consumed, autolag='AIC')\n",
    "pd.Series(t[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721c0adfc5e3e6fb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We can see that $p > 0.05$, hence the data is not stationary. We need to implement some sort of differencing to make the data stationary.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752884daae2e1e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:12:29.058795Z",
     "start_time": "2023-09-25T18:12:28.546183Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train['differenced_values'] = train['power_consumed'].diff(1)\n",
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e050c00ea1d79b4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:12:29.061054Z",
     "start_time": "2023-09-25T18:12:28.555516Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "t = sm.tsa.adfuller(train.differenced_values, autolag='AIC')\n",
    "pd.Series(t[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2c425ca6f94e51",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Single order differencing seems to work here! We can see that $p < 0.05$ now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c87d41763e08f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Decomposition of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff8f5edcb4db15d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:12:29.062295Z",
     "start_time": "2023-09-25T18:12:28.599556Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "s = sm.tsa.seasonal_decompose(train.power_consumed, period=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ae70f65b50dde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:12:29.062636Z",
     "start_time": "2023-09-25T18:12:28.611213Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "s.seasonal.plot(figsize=(20,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4981ec365a47a21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:12:29.404954Z",
     "start_time": "2023-09-25T18:12:28.868543Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "s.trend.plot(figsize=(20,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca1d82b7c2559b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:12:29.482348Z",
     "start_time": "2023-09-25T18:12:29.013237Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "s.resid.plot(figsize=(20,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e454be0d03d2a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## ARIMAX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca6c10379adce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:12:29.482549Z",
     "start_time": "2023-09-25T18:12:29.175538Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e95b2e5f0b663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:12:29.482748Z",
     "start_time": "2023-09-25T18:12:29.178384Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_acf(train.differenced_values,lags=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e191ff75f129f774",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:12:29.483132Z",
     "start_time": "2023-09-25T18:12:29.252872Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_pacf(train.power_consumed,lags=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885eed28baf378f5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Here, we define an algorithm which takes in a range of values of p, d, q and calculates the AIC metric on a vanilla ARIMA model. \n",
    "\n",
    "The **Akaike Information Criterion (AIC)** is a statistical measure used for model selection and comparison in the context of regression analysis and time series modeling.\n",
    "\n",
    "AIC quantifies the trade-off between a model's goodness of fit and its complexity, penalizing models with too many parameters. It is employed to choose the best-fitting model among a set of candidate models. The model with the lowest AIC value is typically preferred because it represents a good balance between explaining the data and avoiding overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa8f71e031ce724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:13:39.864800Z",
     "start_time": "2023-09-25T18:12:29.395706Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Finding the best value for ARIMA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import itertools \n",
    "p=q=range (0,8)\n",
    "d = range(0,2)\n",
    "pdq = list(itertools.product (p, d, q))\n",
    "\n",
    "store = {}\n",
    "for param in pdq:\n",
    "       try:\n",
    "              model_arima = sm.tsa.arima.ARIMA (train.power_consumed, order = param)\n",
    "              model_arima_fit = model_arima.fit()\n",
    "              store[param] =  model_arima_fit.aic  \n",
    "              #print(param, model_arima_fit.aic)\n",
    "       except:\n",
    "              continue\n",
    "          \n",
    "sorted_dict = dict(sorted(store.items(), key=lambda item: item[1]))\n",
    "print(sorted_dict)\n",
    "# The Akaike information criterion (AIC) is an estimator of in-sample prediction error and thereby relative quality of\n",
    "# statistical models for a given set of data\n",
    "# It's like the mean squared error in Regression - The smaller the number, the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ee60451b2ecfff",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "endog = train['power_consumed']\n",
    "exog = sm.add_constant(train[['weather_index', 'holiday_index',\n",
    "       'maximum_temperature', 'humidity', 'wind_velocity', 'pressure',\n",
    "       'rainfall']])\n",
    "\n",
    "mod = sm.tsa.statespace.SARIMAX(endog=endog, exog=exog, order=(7,1,7))\n",
    "model_fit = mod.fit()\n",
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc17250db4b8c176",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Plotting the predicted values on the train set - shows a decent prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a82c26ee725bc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:13:44.795635Z",
     "start_time": "2023-09-25T18:13:44.495437Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train['power_consumed'].plot(figsize=(25,10))\n",
    "model_fit.fittedvalues.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aafa08cbec45bb3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "With this piece of code, we shall perform model inference. \n",
    "We'll use our hold-out test set for this. Using the exogenous variables, we'll provide input into our fitted ARIMAX model, and obtain the predcitions for `power_consumed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215348363598ed64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:13:44.806542Z",
     "start_time": "2023-09-25T18:13:44.793927Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "predict = model_fit.predict(start = len(train),end = len(train)+len(test)-1,exog = sm.add_constant(test[['weather_index', 'holiday_index', 'maximum_temperature', 'humidity', 'wind_velocity', 'pressure', 'rainfall']]))\n",
    "test['predicted'] = predict.values\n",
    "test.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea435a7a097dc18",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We've defined 2 metrics here - MAE and MAPE, to quantify our loss here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3b4f1fe6707908",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:13:44.813510Z",
     "start_time": "2023-09-25T18:13:44.807604Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "MAE = mean_absolute_error(test[\"power_consumed\"], test[\"predicted\"])\n",
    "RMSE = math.sqrt(mean_squared_error(test[\"power_consumed\"], test[\"predicted\"]))\n",
    "print(\"MAE:\", MAE)\n",
    "print(\"RMSE:\", RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e7f9725653a7b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We plot the predicted values vs. the actual values for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1fcc0e39e3d7ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:13:45.386123Z",
     "start_time": "2023-09-25T18:13:44.813834Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test['power_consumed'].plot(figsize=(25,10),color = 'red')\n",
    "test['predicted'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1d0fb26323bcd3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Question 1: Your task is to use the above learnings, and apply a SARIMAX model. Do reuse the code, identify a suitable seasonal order, and experiment to find the best performing model! Also, provide your reasoning for choosing your seasonal order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade7a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['power_consumed'])\n",
    "plt.title('Electricity Consumption Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Power Consumed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41a201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['power_consumed_diff'] = df['power_consumed'].diff()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4871d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(df['power_consumed_diff'], lags=30)\n",
    "plot_pacf(df['power_consumed_diff'], lags=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a0fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Define the range for seasonal p, d, q, and s (seasonal period)\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(range(0, 2), range(0,2), range(0, 2)))]\n",
    "\n",
    "store = {}\n",
    "for param_seasonal in seasonal_pdq:\n",
    "    try:\n",
    "        mod = sm.tsa.statespace.SARIMAX(endog=endog, exog=exog, order=(7, 1, 7), seasonal_order=param_seasonal)\n",
    "        model_fit = mod.fit()\n",
    "        store[param_seasonal] = model_fit.aic\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "sorted_seasonal_dict = dict(sorted(store.items(), key=lambda item: item[1]))\n",
    "sorted_seasonal_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462992d82f4f569e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "> Hint: In your model definition step, you'll have to provide a `seasonal_order` parameter along with `order`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825804370c19263d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640555232f61b6ef",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture in deep learning. LSTMs are designed to address the vanishing gradient problem in traditional RNNs, allowing them to effectively capture and model long-range dependencies in sequential data. They have become a crucial tool for tasks like natural language processing, time series forecasting, and sequential pattern recognition.\n",
    "\n",
    "LSTMs are often used to effectively model complicated time-series problems, so we'll explore this further.\n",
    "\n",
    "For the scope of this worksheet, we'll only use the target variable and it's lags as input to the LSTM. However, you're encouraged to explore how the entire input dataset can be modeled as input to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98dd6221a6fa263",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Our first task would be to convert our time-series forecasting problem, into a supervised learning problem. Any ideas on how we can achieve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcaf920c88dde48",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's first learn the distinction between a time series, and a supervised learning problem.\n",
    "\n",
    "A time series is a sequence of numbers that are ordered by a time index. This can be thought of as a list or column of ordered values.\n",
    "\n",
    "A supervised learning problem comprises input patterns (X) and output patterns (y), such that an algorithm can learn how to predict the output patterns from the input patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f221d831eccf37f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Pandas has a `shift()` function, that we can use to extract **lags** from the target variable. In essence, we want to somehow bring a X->y relation with respect to the target variable, while retaining the time component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a766685f1772edc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "So, what are *lags*?\n",
    "\n",
    "Lags refer to the practice of shifting a time series data point or variable backward in time by a certain number of time units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfae281b8983ac4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "If you're able to understand where this is going now....\n",
    "\n",
    "We're essentially going to create a mapping such that : \n",
    "\n",
    "`var(t - 1) -> var(t)`; which resembles `X -> y` !\n",
    "\n",
    "We can go further here, and take more lags, such as `var(t - 2), var(t - 3)`, etc.!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4155de947c5394b0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Incase you found this prelude a little difficult to follow, consider going through a more detailed write-up here: \n",
    "\n",
    "[Machine Learning Mastery's Blog on converting time series to supervised learning](https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3618d690b248cab0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Applying LSTM to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d264e0b03ca223",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:14:01.633811Z",
     "start_time": "2023-09-25T18:14:01.551364Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "dataframe = df.loc[:,'power_consumed']\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7943bec0255741",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "For this demonstration purpose, we'll use lags of 7 days, and convert it into a supervised learning problem. \n",
    "\n",
    "Here's a function that's borrowed from the aforementioned blog, that helps us in converting the time series to a supervised problem - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6f86077d20ef61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:14:01.633854Z",
     "start_time": "2023-09-25T18:14:01.557635Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9526e73798e54ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:14:01.634075Z",
     "start_time": "2023-09-25T18:14:01.560209Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "new_dataset = series_to_supervised(dataset, 7,1)\n",
    "new_dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b970d1160a109f12",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Just to give a taste of multivariate time-series forecasting using LSTMs - we'll use `weather_index` and `holiday_index` in our input to the model as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8a6cfa4726c5fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:14:01.634117Z",
     "start_time": "2023-09-25T18:14:01.566528Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "new_dataset['weather_index'] = df.weather_index.values[7:]\n",
    "new_dataset['holiday_index']= df.holiday_index.values[7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0c3eac98bb2f38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:14:01.634168Z",
     "start_time": "2023-09-25T18:14:01.570075Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "new_dataset = new_dataset.reindex(['weather_index', 'holiday_index','var1(t-7)', 'var1(t-6)', 'var1(t-5)', 'var1(t-4)', 'var1(t-3)','var1(t-2)', 'var1(t-1)', 'var1(t)'], axis=1)\n",
    "new_dataset = new_dataset.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9e1af188624813",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Remember - we cannot use DataFrames, as LSTMs (and most other deep learning models) only accept tensors as input!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c6243e85cc268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:14:01.634392Z",
     "start_time": "2023-09-25T18:14:01.576367Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "type(new_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ca84c456f0c9b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We'll scale our features between 0 and 1 - this would be to help the process of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde19a0c58751be2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:14:01.634599Z",
     "start_time": "2023-09-25T18:14:01.583500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "new_dataset = scaler.fit_transform(new_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db330ffcb9d8edf1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We'll split our dataset into train and test, as done for ARIMAX. Remember, it is still inherently a time series problem, so we will not use a random split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f7ef8a005f91cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:14:01.634642Z",
     "start_time": "2023-09-25T18:14:01.586755Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_lstm = new_dataset[:(len(new_dataset)-30), :]\n",
    "test_lstm = new_dataset[(len(new_dataset)-30):len(new_dataset), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad6c2a751e9c47",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We adjust the labels, such that `train_X` and `test_X` contain the features, and `train_Y`, `test_Y` contain the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae884cd6d3e768b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:14:01.634700Z",
     "start_time": "2023-09-25T18:14:01.589065Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_X, train_y = train_lstm[:, :-1], train_lstm[:, -1]\n",
    "test_X, test_y = test_lstm[:, :-1], test_lstm[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a107909569fb9f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The input to a LSTM is 3D - in this format: (samples, timesteps, features). We'll go ahead and reshape our train and tests sets as such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c4a87b1ec6692d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:14:01.643230Z",
     "start_time": "2023-09-25T18:14:01.591515Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227e9e8cc34bb8cd",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## LSTM Modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aec9aa64c67e91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:14:02.831019Z",
     "start_time": "2023-09-25T18:14:01.595916Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, verbose=2, shuffle=False)\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23b8457fc54b96f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## LSTM Inferencing (Model Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dcdc8344a144c0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Since we went through the whole charade of Scaling our values - making a prediction isn't completely straightforward.\n",
    "We need to invert the scaling, in order to obtain the correct forecast value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81d34c3806a4e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:14:03.019782Z",
     "start_time": "2023-09-25T18:14:02.832698Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c043e66f4c00b43d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:14:03.025127Z",
     "start_time": "2023-09-25T18:14:03.008845Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_X = test_X.reshape(test_X.shape[0], test_X.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5006b5453ba726",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:14:03.025384Z",
     "start_time": "2023-09-25T18:14:03.014490Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "inv_yhat = np.concatenate((yhat, test_X), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e666a10b19686d2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:14:03.060127Z",
     "start_time": "2023-09-25T18:14:03.020601Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = np.concatenate((test_y, test_X), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ae4956743fd6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Checking the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f237d057bd01be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:15:48.206467Z",
     "start_time": "2023-09-25T18:15:48.188163Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "act = [i[0] for i in inv_y] # last element is the predicted power consumption\n",
    "pred = [i[0] for i in inv_yhat] # last element is the actual power consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ca92faef88400",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T18:15:48.622527Z",
     "start_time": "2023-09-25T18:15:48.604292Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "mae = mean_absolute_error(act, pred)\n",
    "rmse = math.sqrt(mean_squared_error(act, pred))\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b63b20eb34d6014",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Question 2: Can we use accuracy as a metric for this particular problem? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea79d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No, we cannot use accuracy as a metric for this particular problem. Accuracy is a binary metric, meaning that it only takes into account whether a prediction is correct or incorrect. However, in the case of time series forecasting, we are interested in the magnitude of the error, not just whether the prediction is correct or incorrect. Time series data has an inherent order and dependency between observations, making accuracy a misleading metric.\n",
    "#For time series forecasting, metrics like Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) are more suitable. MAE gives the average magnitude of errors between predicted and actual values, while RMSE penalizes larger errors more heavily. These metrics are sensitive to the magnitude and direction of errors, making them valuable for evaluating the performance of forecasting models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c9dad5d249f42",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Question 3: When can LSTMs outperform ARIMA, ARIMAX or SARIMAX models? Is it worth the computational expense to fit an LSTM over a traditional time series model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89666d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTMs can outperform ARIMA, ARIMAX, and SARIMAX models when the data is nonlinear and/or non-stationary. ARIMA, ARIMAX, and SARIMAX models are linear models, and they assume that the data is stationary. LSTMs, on the other hand, are non-linear models, and they can handle non-stationary data.\n",
    "#However, LSTMs are computationally more expensive to fit than ARIMA, ARIMAX, and SARIMAX models. Therefore, it is important to weigh the potential benefits of using an LSTM against the computational cost.\n",
    "#If the data is linear and stationary, then an ARIMA, ARIMAX, or SARIMAX model may be sufficient. However, if the data is nonlinear and/or non-stationary, then an LSTM may be a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a90930a17a0c2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Question 4: What can you elucidate about the interpretability of ARIMA/ARIMAX vs LSTMs?\n",
    "\n",
    "ARIMA, ARIMAX, and SARIMAX models are more interpretable than LSTMs. This is because ARIMA, ARIMAX, and SARIMAX models are based on statistical theory, and they have a clear mathematical formulation.\n",
    "LSTMs, on the other hand, are black-box models. This means that it is difficult to understand how LSTMs make predictions. This is because LSTMs are complex neural networks with many parameters.\n",
    "If interpretability is important, then an ARIMA, ARIMAX, or SARIMAX model may be a better choice. However, if interpretability is not as important, then an LSTM may be a better choice.\n",
    "In general, LSTMs are a good choice for time series forecasting when the data is nonlinear and/or non-stationary, and when interpretability is not as important. ARIMA, ARIMAX, and SARIMAX models are a good choice for time series forecasting when the data is linear and stationary, and when interpretability is important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da941424587b29",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Hint: Think black-box models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b3513e71988f25",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Congratulations on making it to the end of the worksheet! I hope you have a much better understanding of modeling the time-series workflow, and applications of Deep Learning methods too!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c70a48b94c14a26",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Incase you want to explore further, Facebook Prophet is a great time series model as well to have in your toolbox!\n",
    "## Read more [here!](https://www.kaggle.com/code/prashant111/tutorial-time-series-forecasting-with-prophet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
