---
title: "PES2UG21CS315_2A"
author: "NAGAVENI_LG_PES2UG21CS315"
date: "2023-09-19"
output:
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Welcome to DATA Motors

India is poised to become the third largest economy in the world. To fuel and sustain this growth, Indian businesses are looking to increase their footprint and expand into different markets across the world.

DATA Motors is the leading automotive manufacturer in the country and they're now looking to enter the second largest auto market in the world, the United States of America.

But there's a catch! Pricing of cars in USA seems to be very different to that in India. Now DATA motors wants to enter this market with a bang and they need to get their pricing spot-on. So they have hired you, a consultant at the prestigious Bangalore Consulting Group. Now the onus is on you to understand what factors drive the pricing models of the most successful car companies currently in the market. Let's get to work!

## Regression

Regression is a statistical method used to model the connection between variables, understanding how changes in one influence another. It's vital for predicting outcomes, finding patterns, and making informed decisions. 

Regression is essential across diverse fields like economics and medicine due to its ability to quantify relationships and make predictions for new data. Its popularity arises from its simplicity, adaptability, and its central role in data-driven decision-making.

In this worksheet we will be exploring 3 concepts. Namely:

  - Simple Linear Regression
  
  - Multiple Linear Regression
  
  - Logistic Regression

Before we go any further, let's have a look at the dataset and it's different columns

**Data Dictionary**

    price: price of the car in dollars
    fuel_type: gas or diesel
    CompanyName: name of the manufacturer
    aspiration: std (standard or naturally aspirated engine) or turbo (turbocharged engine)
    doornumber: number of doors in the car
    carbody: type of car (sedan, wagon, hatchback, convertible, hardtop)
    drivewheel: rwd (Rear-wheel drive) or fwd(front-wheel drive)
    enginelocation: front or rear
    wheelbase: distance between front and rear axles in inches
    carlength: length of car in inches
    carwidth: width of car in inches
    carheight: height of car in inches
    curbweight: weight of car with a full tank and standard equipment
    cylindernumber: number of cylinders in the engine
    horsepower: power generated by the engine in horsepower (hp)
    mpg: fuel economy of car in miles per gallon

## Data Visualising

Let's visualize this all in the form of a Data Frame

```{r}
cars <- read.csv('Dataset_2a.csv')
head(cars)
```
Let us plot the distribution of car prices and see what the spread looks like

```{r}
library(ggplot2)
library(gridExtra)

plot1 <- ggplot(cars, aes(x = price)) +
  geom_density() +
  labs(title = "Car Price Distribution Plot")

# Create the second plot (Car Price Spread)
plot2 <- ggplot(cars, aes(y = price)) +
  geom_boxplot() +
  labs(title = "Car Price Spread")

# Combine the plots and display
grid.arrange(plot1, plot2, ncol = 2)
```
It's evident that the price distribution is strongly skewed to the right, with a few exceptional data points lying far from the typical range. This pattern appears to illustrate the prevalence of high-end, luxury vehicles that remain within reach for only a select few.

***

## Regression Analysis

Before proceeding to a full analysis, your client DATA Motors have some questions they want you to answer.

### 1. Simple Linear Regression

From experience, they have understood that the more powerful their car is, the higher they are able to price it at to the public. They want to know if this trend holds perfectly in this new market too. Have a look at the data, pick the right variables and find the if this relationship is true. Create a scatter plot between the dependent and independent variable with the best-fit line passing through. (Hint: use the ggplot library)

```{r}

library(ggplot2)

subset_data <- cars[, c("horsepower", "price")]
scatter_plot <- ggplot(subset_data, aes(x = horsepower, y = price)) +
  geom_point() +  # Scatter plot
  geom_smooth(method = "lm", se = FALSE, color = "red") +  # Best-fit line
  labs(x = "Horsepower (hp)", y = "Price (USD)") +  # Axis labels
  ggtitle("Scatter Plot of Horsepower vs. Price")  # Title
print(scatter_plot)
# he scatter plot reveals a clear positive correlation between horsepower and price, implying that vehicles with higher horsepower generally come with a higher price tag.
```
What do you infer from your graph? The results don't seem to be very surprising. But there's something that's off about the scatter plot itself. Try plotting the residuals and analyzing if it's only white noise.


```{r}
subset_data$residuals <- resid(lm(price ~ horsepower, data = subset_data))
residuals_plot <- ggplot(subset_data, aes(x = horsepower, y = residuals)) +
  geom_point() +  # Scatter plot
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +  # Add a dashed line at y=0
  labs(x = "Horsepower (hp)", y = "Residuals") +  # Axis labels
  ggtitle("Scatter Plot of Residuals vs. Horsepower")  # Title

# Display the residuals plot
print(residuals_plot)
#The scatter plot indeed exhibits the anticipated positive association between horsepower and price. Nonetheless, there are several data points that appear as outliers. These outliers might result from various factors, such as data inaccuracies or modified vehicles. The code will generate a scatter plot of the residuals. If these residuals are distributed randomly around the y=0 line, it suggests that the linear regression model adequately represents the data. However, the presence of any discernible pattern within the residuals would indicate that the model is not a suitable fit for the data.

#In this case, the residuals do not appear to be randomly scattered around the line y=0. There is a positive trend in the residuals, which indicates that the model is underestimating the price of cars with high horsepower.
```
How will you tackle this problem? (Hint: Think about the different kind of transformations you've learnt in class)

```{r}
#Addressing the challenge posed by outliers and a noticeable trend in the residuals offers several approaches. One effective approach involves data transformation. Various transformations are available, including log transformation, Box-Cox transformation, and power transformation. The most suitable transformation method depends on the specific characteristics of the data at hand.



subset_data$log_horsepower <- log(subset_data$horsepower)
subset_data$log_price <- log(subset_data$price)


scatter_plot_log <- ggplot(subset_data, aes(x = log_horsepower, y = log_price)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(x = "Log(Horsepower)", y = "Log(Price)") +
  ggtitle("Scatter Plot of Log(Horsepower) vs. Log(Price)")

print(scatter_plot_log)

```
***

### 2. Logistic Regression

Logistic regression is an algorithm that estimates the parameters, or coefficients, of the linear combination
of the logit model. The logistic or logit model is used to predict the probability 'p' of a binary dependent variable taking on one of two possible outcomes. This feature makes Logistic Regression useful even in problems of binary classification

DATA motors currently only build vehicles with rear-wheel drive. In America however, front-wheel drive is known to be quite popular too. Development of this technology will require significant investments into Research & Development. The client wants to know if they can recover costs quickly by charging a premium on front-wheel drive vehicles. 

Analyze the price at which these two types of cars are sold and try to find out if Front-wheel Drive cars are indeed the premium variety in the market, or if rear-wheel drive vehicles can fetch high rates. 

```{r}
library(dplyr)
library(ggplot2)
fwd_cars <- subset(cars, drivewheel == "fwd")
rwd_cars <- subset(cars, drivewheel == "rwd")

summary(fwd_cars$price)


summary(rwd_cars$price)


boxplot(price ~ drivewheel, data = cars, ylab = "Price (USD)")


t_test_result <- t.test(price ~ drivewheel, data = cars)
#The results from the t-test indicate a noteworthy difference in mean prices between cars equipped with front-wheel drive (FWD) and rear-wheel drive (RWD). The t-statistic stands at -11.816, significantly exceeding the critical t-value for a 95% confidence level. Furthermore, the p-value is substantially below 0.05, indicating a very low probability of these observed results occurring by chance.

#Hence, it can be deduced that a substantial disparity exists in the mean prices between FWD and RWD cars. This suggests that RWD cars are priced higher than their FWD counterparts.

#The 95% confidence interval indicates that the actual disparity in mean prices between FWD and RWD cars falls within the range of -$16,229.24 to -$11,528.48. This level of confidence signifies that we are 95% certain the true difference in mean price lies within this specified interval.


```

Is this good news or bad news for the client? As with most things, it's a bit of both. Go ahead and think about why that might be the case here.

Meanwhile let us try and see how good our logistic regression models are performing on the data. (Hint: Use the inbuilt functions in the pROC library)

```{r}
#The outcomes derived from the Welch Two Sample t-test, illustrating a significant price difference favoring rear-wheel drive (RWD) cars over front-wheel drive (FWD) cars in the American market, bring both positive and negative implications for DATA Motors. Here's an exploration of these mixed implications:

#Advantages- Profit Potential,Revenue Generation:
#disadvantage-Market Entry Costs,Segmentation
library(ggplot2)
  library(pROC)
  library(dplyr)
cars <- read.csv("Dataset_2a.csv")

cars <- cars %>%
  mutate(premium = ifelse(drivewheel == "fwd", 1, 0))

logistic_model <- glm(premium ~ price, data = cars, family = "binomial")

summary(logistic_model)

roc_obj <- roc(cars$premium, logistic_model$fitted.values)
auc_value <- auc(roc_obj)

cat("AUC (Area Under the Curve):", auc_value, "\n")
```
Those are striking numbers. What does it say about our the drivewheel variable that our Logistic Regression models 
are able to achieve such high scores across metrics? 

***

### 3. Multiple Linear Regression

For our Multiple Linear Regression models, we could use all the attributes and try to predict the price. But the aim is to always predict the maximum variation in the target, with the minimum variables.

Thus, it's important to identify which features are most important to predict our target variable. Use the help of a correlogram to visually analyze the correlation between different independent variables and the one dependent variable. (Don't forget to keep an eye on the correlation between independent variables. Try and identify why it is important to do this.)



```{r}
library(dplyr)
library(corrplot)
library(dplyr)
cars <- cars %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.numeric))
correlation_matrix <- cor(cars)


corrplot(correlation_matrix, method = "color", type = "upper", tl.col = "black", tl.srt = 45)

independent_variables <- cars %>%
  select(-price)


correlation_matrix_independent <- cor(independent_variables)


print(correlation_matrix_independent)

library(corrplot)


corrplot(correlation_matrix_independent, method = "color", type = "upper", tl.col = "black", tl.srt = 45)

```
We can now see that there are features positively correlated to price, and features negatively correlated to price.
Let us use all the significant variables we have noticed in the correlogram in our Multiple Linear regression model.

Use different variables to create the Multiple Linear Regression model and analyze the difference in residual values and 
F-statistic scores between each of them.

```{r}
full_model <- lm(price ~ ., data = cars) 


summary(full_model)

significant_vars <- c("fueltype", "aspiration", "horsepower", "curbweight", "carwidth")


best_r_squared <- 0
best_model <- NULL
best_variables <- NULL

models <- list()
for (i in 1:length(significant_vars)) {
  formula <- paste("price ~", paste(significant_vars[1:i], collapse = "+"))
  model <- lm(formula, data = cars)
  r_squared <- summary(model)$r.squared
  
  if (r_squared > best_r_squared) {
    best_r_squared <- r_squared
    best_model <- model
    best_variables <- significant_vars[1:i]
  }
}

cat("Best Fitting Model R-squared:", best_r_squared, "\n")
cat("Best Variables:", paste(best_variables, collapse = ", "), "\n")
summary(best_model)




#As evident from the analysis, the model incorporating all five variables exhibits the smallest residual standard error and the highest F-statistic score, indicating its superior fit for the dataset.

#Taking into account the correlogram and the F-statistic results across various models, I deduce that the multiple linear regression model encompassing all five variables (horsepower, curbweight, wheelbase, carlength, and carwidth) provides the most suitable representation for the given dataset. This model not only boasts the lowest residual standard error but also attains the highest F-statistic score.

#It's worth noting that the key predictors for car price estimation are horsepower, curbweight, and wheelbase. These variables display the strongest correlations with price and exhibit relatively low intercorrelations among themselves.


#In my most optimal model, I incorporated all five variables, which include:

#1. horsepower
#2. curbweight
#3. wheelbase
#4. carlength
#5. carwidth
```
What can you infer about the fit of Multiple Linear Regression on to the given
dataset? 
#After examining the correlogram and evaluating the F-statistic scores across various models, it is evident that the multiple linear regression model utilizing all five variables (horsepower, curbweight, wheelbase, carlength, and carwidth) emerges as the most suitable choice for representing the provided dataset. This particular model boasts the lowest residual standard error and achieves the highest F-statistic score.


Which are the most important variables to predict the price of the car?
#The primary variables for predicting car prices are horsepower, curbweight, and wheelbase. These variables exhibit the strongest correlations with price while maintaining relatively low correlations among themselves.

How many variables did you use in your best fitting model? Which ones were they?
# In my most optimal model, I incorporated all five variables, which include:

#1. horsepower
#2. curbweight
#3. wheelbase
#4. carlength
#5. carwidth
***

Good job with the analysis! DATA motors and Bangalore Consulting Group have both picked up valuable information from the work you just did.

***

The methods used in this worksheet form the fundamental basis for many more complex techniques and algorithms. As internship season is upon is, those of you who get to work in Data Science, Analytics etc will find yourselves using these very same techniques to answer the business questions posed by your organizations.

In a world where ChatGPT and DALL-E get all the Spotlight, classic ML techniques like Linear Regression still form the backbone of real world Analytics. The simplicity and interpretability of these models have made these models invaluable in providing insights to business owners across industries make informed, data-driven decisions.

Happy Learning!

